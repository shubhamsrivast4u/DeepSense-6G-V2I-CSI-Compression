{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Select the GPU index\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "import spacy\n",
    "from scipy.io import savemat\n",
    "from scipy import stats\n",
    "import dill as pickle\n",
    "import thop\n",
    "from torch_challenge_dataset import DeepVerseChallengeLoaderTaskThree\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f11ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "onoffdict={'GPS': True, 'CAMERAS': True, 'RADAR': True}\n",
    "reduction = 4\n",
    "num_H = 64\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8236d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a3e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c6170",
   "metadata": {},
   "source": [
    "# Utils and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab275daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_abs_reshape(y, csi_std=2.8117975e-06, target_std=1.0):\n",
    "    y = torch.abs(y)\n",
    "    y=(y/csi_std)*target_std\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73152fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_reshape( y, csi_std=2.5e-06, target_std=1):\n",
    "        ry = torch.real(y)\n",
    "        iy= torch.imag(y)\n",
    "        oy=torch.cat([ry,iy],dim=1)\n",
    "        #scaling\n",
    "        oy=(oy/csi_std)*target_std\n",
    "        return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384e658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b783bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_parameters(model):\n",
    "    total_param  = []\n",
    "    for p1 in model.parameters():\n",
    "        total_param.append(int(p1.numel()))\n",
    "    return sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee9b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    # Convert image to float tensor\n",
    "    image = image.float()\n",
    "    # Normalize the image\n",
    "    image /= 255.0\n",
    "    # ImageNet mean values # ImageNet standard deviation values\n",
    "    trans=T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) \n",
    "    image=trans(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2581a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_coordinates_batch(x_cor, y_cor):\n",
    "    y_pix = torch.zeros_like(x_cor)\n",
    "    x_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition1 = y_cor < -4\n",
    "    condition2 = (y_cor >= -4) & (y_cor < -1)\n",
    "    condition3 = (y_cor >= -1) & (y_cor < 1)\n",
    "    condition4 = (y_cor >= 1) & (y_cor < 4)\n",
    "    condition5 = y_cor >= 4\n",
    "\n",
    "    y_pix[condition1] = 100 + (250 - 100) * ((x_cor[condition1] - 80) / (125 - 80))\n",
    "    y_pix[condition2] = 100 + (250 - 100) * ((x_cor[condition2] - 80) / (125 - 80))\n",
    "    y_pix[condition3] = 100 + (250 - 100) * ((x_cor[condition3] - 80) / (125 - 80))\n",
    "    y_pix[condition4] = 100 + (210 - 100) * ((x_cor[condition4] - 80) / (125 - 80))\n",
    "    y_pix[condition5] = 100 + (190 - 100) * ((x_cor[condition5] - 80) / (125 - 80))\n",
    "\n",
    "    x_pix[condition1] = (y_pix[condition1] - 30) / 1.35\n",
    "    x_pix[condition2] = (y_pix[condition2] - 45) / 0.85\n",
    "    x_pix[condition3] = (y_pix[condition3] - 55) / 0.70\n",
    "    x_pix[condition4] = (y_pix[condition4] - 65) / 0.60\n",
    "    x_pix[condition5] = (y_pix[condition5] - 65) / 0.5\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c4777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_coordinates_batch(x_cor, y_cor):\n",
    "    x_pix = torch.zeros_like(x_cor)\n",
    "    y_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition = y_cor < 0\n",
    "    x_pix[condition] = 256 * ((x_cor[condition] - 119) / (139 - 119))\n",
    "    x_pix[~condition] = 256 * ((x_cor[~condition] - 112) / (146 - 113))\n",
    "    \n",
    "    y_pix = 175 + (100 - 175) * ((y_cor - (-7)) / ((7) - (-7)))\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ce57e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_coordinates_batch(x_cor, y_cor):\n",
    "    y_pix = torch.zeros_like(x_cor)\n",
    "    x_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition1 = y_cor < -4\n",
    "    condition2 = (y_cor >= -4) & (y_cor < -1)\n",
    "    condition3 = (y_cor >= -1) & (y_cor < 1)\n",
    "    condition4 = (y_cor >= 1) & (y_cor < 4)\n",
    "    condition5 = y_cor >= 4\n",
    "\n",
    "    y_pix[condition1] = 250 + (100 - 250) * ((x_cor[condition1] - 125) / (200 - 125))\n",
    "    y_pix[condition2] = 250 + (100 - 250) * ((x_cor[condition2] - 125) / (200 - 125))\n",
    "    y_pix[condition3] = 250 + (100 - 250) * ((x_cor[condition3] - 125) / (200 - 125))\n",
    "    y_pix[condition4] = 210 + (100 - 210) * ((x_cor[condition4] - 125) / (200 - 125))\n",
    "    y_pix[condition5] = 190 + (100 - 190) * ((x_cor[condition5] - 125) / (200 - 125))\n",
    "\n",
    "    x_pix[condition1] = -(y_pix[condition1] - 370) / 1.25\n",
    "    x_pix[condition2] = -(y_pix[condition2] - 285) / 0.87\n",
    "    x_pix[condition3] = -(y_pix[condition3] - 250) / 0.73\n",
    "    x_pix[condition4] = -(y_pix[condition4] - 210) / 0.55\n",
    "    x_pix[condition5] = -(y_pix[condition5] - 190) / 0.45\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c98e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_image_batch(images, center_x, center_y, output_size, bounded=False):\n",
    "    batch_size = images.size(0)\n",
    "    \n",
    "    if bounded == 'left':\n",
    "        top = torch.clamp(center_y - output_size[0] // 2, 0, None)\n",
    "        left = torch.clamp(center_x - output_size[1] // 2, 0, None)\n",
    "    elif bounded == 'right':\n",
    "        bottom = center_y + output_size[0] // 2\n",
    "        right = torch.clamp(center_x + output_size[1] // 2, None, 250)\n",
    "        top = torch.clamp(bottom - output_size[0], 0, None)\n",
    "        left = right - output_size[1]\n",
    "    else:\n",
    "        top = center_y - output_size[0] // 2\n",
    "        left = center_x - output_size[1] // 2\n",
    "\n",
    "    resize_transform = transforms.Resize((output_size))\n",
    "    cropped_images = [TF.crop(image, int(top[i].item()), int(left[i].item()), output_size[0], output_size[1]) \n",
    "                        for i, image in enumerate(images)]\n",
    "    cropped_images = torch.stack([resize_transform(image) for image in cropped_images])\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93af7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imgs(gps, img_1, img_2, img_3, crop_size = (150,150)):\n",
    "    x_cor = gps[:,0]\n",
    "    y_cor = gps[:,1]\n",
    "\n",
    "    x_pix,y_pix = left_coordinates_batch(x_cor, y_cor)\n",
    "    img_1 = center_image_batch(img_1, x_pix.to(torch.int), y_pix.to(torch.int), crop_size, 'left')\n",
    "\n",
    "    x_pix,y_pix = center_coordinates_batch(x_cor, y_cor)\n",
    "    img_2 = center_image_batch(img_2, x_pix.to(torch.int), y_pix.to(torch.int), crop_size)\n",
    "\n",
    "    x_pix,y_pix = right_coordinates_batch(x_cor, y_cor)\n",
    "    img_3 = center_image_batch(img_3, x_pix.to(torch.int), y_pix.to(torch.int), crop_size, 'right')\n",
    "\n",
    "    return img_1, img_2, img_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516ccee",
   "metadata": {},
   "source": [
    "Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25de8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpsdata(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gps_fc = nn.Linear(2, 16)\n",
    "        self.gps_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, gps):  \n",
    "        gps = gps.to(torch.float32)\n",
    "\n",
    "        x, y = gps[:,0], gps[:,1]\n",
    "        x_normd = (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
    "        y_normd = (y - torch.min(y)) / (torch.max(y) - torch.min(y))\n",
    "        gps_normd = torch.stack([x_normd,y_normd],dim=1)\n",
    "\n",
    "        gps_out = self.gps_fc(gps_normd)  \n",
    "        gps_out = self.gps_relu(gps_out)\n",
    "        return gps_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a26d3e",
   "metadata": {},
   "source": [
    "Radar Data Processing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e65f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class radardata(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(radardata, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 1, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.lr1=nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        self.encoder_fc = nn.Linear(256,16)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,1,512,128)\n",
    "        x = (x - 5.1838e-06) / (28.0494 - 5.1838e-06)\n",
    "        out = self.pool1(self.dropout(self.conv1(x)))\n",
    "        out = self.pool2(self.dropout(self.conv2(out))).view(x.size(0), -1)\n",
    "        out = self.dropout(self.encoder_fc(out))\n",
    "        out = self.lr1(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004a71e",
   "metadata": {},
   "source": [
    "Camera Data Processing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dce3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cameradata(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 1, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.lr1=nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        self.encoder = nn.Linear(1*81,16)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, cam):  \n",
    "        cam = normalize_image(cam).to(torch.float32)\n",
    "        out = self.pool1(self.dropout(self.conv1(cam)))\n",
    "        out = self.pool2(self.dropout(self.conv2(out)))\n",
    "        out = self.lr1(out).view(-1,1*81)\n",
    "        out = self.dropout(self.encoder(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de38d4b",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c21d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size,stride=1, activation=\"LeakyReLu\"):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        dict_activation ={\"LeakyReLu\":nn.LeakyReLU(negative_slope=0.3,inplace=True),\"Sigmoid\":nn.Sigmoid(),\"Tanh\":nn.Tanh()}\n",
    "        activation_layer = dict_activation[activation]\n",
    "        super(ConvLayer, self).__init__(OrderedDict([\n",
    "            (\"conv\", nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=padding, bias=False)),\n",
    "            (\"bn\", nn.BatchNorm2d(out_planes)),\n",
    "            (\"activation\",activation_layer)\n",
    "        ]))\n",
    "\n",
    "class RefineNetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefineNetBlock, self).__init__()\n",
    "        self.direct = nn.Sequential(OrderedDict([\n",
    "            (\"conv_7x7\", ConvLayer(2, 8, 7, activation=\"LeakyReLu\")),\n",
    "            (\"conv_5x5\", ConvLayer(8, 16, 5, activation=\"LeakyReLu\")),\n",
    "            (\"conv_3x3\",ConvLayer(16,2,3,activation=\"Tanh\"))\n",
    "        ]))\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        out = self.direct(x)\n",
    "        out = self.relu(out + identity)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2f70d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task2Encoder, self).__init__()\n",
    "        total_size, in_channel, w, h = 8192, 2, 64, 64\n",
    "        self.encoder_conv = nn.Sequential(OrderedDict([\n",
    "            (\"conv1_7x7\", ConvLayer(1, 2, 7, activation='LeakyReLu')),\n",
    "            (\"conv2_7x7\",ConvLayer(2,2,7,activation='LeakyReLu'))\n",
    "        ]))\n",
    "        self.encoder_fc = nn.Linear(total_size, total_size // reduction)\n",
    "         \n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.detach().size()\n",
    "        out = self.encoder_conv(x.to(torch.float32))\n",
    "        out = self.encoder_fc(out.view(n, -1))\n",
    "        return out\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf1fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task2Decoder, self).__init__()\n",
    "        total_size, in_channel, w, h = 8192, 2, 64, 64\n",
    "        self.decoder_fc = nn.Linear(total_size // reduction, total_size)\n",
    "        self.decoder_conv = ConvLayer(2, 2, 7, activation=\"Sigmoid\")\n",
    "        self.decoder_refine = nn.Sequential(OrderedDict([\n",
    "            (f\"RefineNet{i+1}\",RefineNetBlock()) for i in range(5)\n",
    "        ]))\n",
    "        self.decoder_sigmoid = nn.Sigmoid()\n",
    "        self.decoder_fc2 = nn.Linear(total_size, total_size//2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, Hencoded):\n",
    "        bs = Hencoded.size(0)\n",
    "        out = self.decoder_fc(Hencoded).view(bs, -1, 64, 64)\n",
    "        out = self.decoder_conv(out)\n",
    "        out = self.decoder_refine(out)\n",
    "        out = self.sig2(self.decoder_fc2(out.view(bs, -1)))\n",
    "        \n",
    "        return out.view(bs, -1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7db01c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Encoder, self).__init__()\n",
    "        \n",
    "\n",
    "        #self.en=task2Encoder(reduction)\n",
    "        #reduction value is already considered in the task2weight_path\n",
    "        # loading preloaded values\n",
    "        self.en=task2Encoder(reduction)\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the parameters to match the batch size\n",
    "        if self.allow_update:\n",
    "            out = self.en(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.en(x)\n",
    "        \n",
    "        encoded_features=out\n",
    "        return encoded_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d8869a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        self.de = task2Decoder(reduction)\n",
    "        \n",
    "        #Layers for auto regression \n",
    "        self.a= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.b= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.c= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.d= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hencoded, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        a = self.a.expand(bs, -1)\n",
    "        b = self.b.expand(bs, -1)\n",
    "        c = self.c.expand(bs, -1)\n",
    "        d = self.d.expand(bs, -1)\n",
    "        out_tminus1=input_autoregressive_features[:,0,:].view(bs,-1)\n",
    "        out_tminus2=input_autoregressive_features[:,1,:].view(bs,-1)\n",
    "        if self.allow_update:\n",
    "            out_t = self.de(Hencoded)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out_t = self.de(Hencoded)\n",
    "        #print(out_t.shape)\n",
    "        out = (out_t.view(bs,-1)) * a + out_tminus1 * b + out_tminus2 * c + d\n",
    "        \n",
    "        autoregressive_features = out\n",
    "        \n",
    "        output = out.view(bs,1, 64, 64)\n",
    "        \n",
    "        return output, autoregressive_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dd4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading weights of baseline model and task 1\n",
    "onoffdictb={'GPS': False, 'CAMERAS': False, 'RADAR': False} #baseline dictionary\n",
    "\n",
    "weight_pathb=f'models/CSINetplustask3/cr{reduction}/gps{onoffdictb[\"GPS\"]}_cam{onoffdictb[\"CAMERAS\"]}_rad{onoffdictb[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a76c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3feea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task1decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gp = gpsdata()\n",
    "        self.rd = radardata()\n",
    "        self.lc = cameradata()\n",
    "        self.cc = cameradata()\n",
    "        self.rc = cameradata()\n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = nn.Linear(16*5, int(num_H/2)*int(num_H/2))\n",
    "            self.output_fc = nn.Linear(int(num_H/2)*int(num_H/2), num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = nn.Linear(16*5, 32)\n",
    "            self.output_fc = nn.Linear(32, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = gps.size(0)\n",
    "        \n",
    "        if onoffdict['GPS']:\n",
    "             gps_out = self.gp(gps)\n",
    "        else:\n",
    "             gps_out = torch.zeros(bs, 16).to(device)\n",
    "        \n",
    "        if onoffdict['RADAR']:\n",
    "            radar_out = self.rd(radar)\n",
    "        else:\n",
    "            radar_out = torch.zeros(bs, 16).to(device)\n",
    "        \n",
    "        if onoffdict['CAMERAS']:\n",
    "            left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "            lc_out = self.lc(left_cam)\n",
    "            cc_out = self.cc(center_cam)\n",
    "            rc_out = self.rc(right_cam)\n",
    "        else:\n",
    "            lc_out = torch.zeros(bs, 16).to(device)\n",
    "            cc_out = torch.zeros(bs, 16).to(device)\n",
    "            rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "        combined = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "        \n",
    "        output = self.linear(combined)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc(output)\n",
    "        output = self.output_relu(output)\n",
    "        output = output.view(output.size(0), 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b9942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d5a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51999f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38166cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c039b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78f34e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"models\" folder exists, create it if it doesn't\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2ba50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#criterion=nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6c938",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b7cd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, device, criterion):\n",
    "    num_test_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse1 = 0\n",
    "        for b,t_x in enumerate(test_loader):\n",
    "            model.ar = [None] * 5 \n",
    "            for time_index,(X, y) in enumerate(t_x):\n",
    "                y_test_reshaped = CSI_abs_reshape(y.to(device))\n",
    "                Xin = CSI_abs_reshape(X[0].to(device))\n",
    "                # Get the input and output for the given time index\n",
    "                y_pred = model(Xin,X[1].to(device),X[2].to(device),X[3].to(device),X[4].to(device),X[5].to(device), time_index, device, is_training=True, onoffdict = onoffdict)\n",
    "                mse0 = criterion(y_pred, y_test_reshaped) \n",
    "                mse1+=mse0  \n",
    "        avg_mse=mse1/(5*num_test_batches)\n",
    "    return avg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5399da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "585716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DeepVerseChallengeLoaderTaskThree(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = torch.tensor([])\n",
    "for b,t_x in enumerate(test_loader):\n",
    "        for time_index,(X, y) in enumerate(t_x):\n",
    "            h = CSI_abs_reshape(y)\n",
    "            h_list = torch.cat([h_list,h])\n",
    "target_loss = torch.mean((torch.abs(h_list) - torch.mean(torch.abs(h_list))) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0501436",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c83fa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 67.2687%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.4927%\n",
      "Mean MSE: 0.3694\n",
      "95% Confidence Interval: (0.3639, 0.3750)\n",
      "Margin of Error: 0.0056\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19646553",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d02138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': True, 'RADAR': False}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd977a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc2ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "483f2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "592d4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a42288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc5534",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a180f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 67.7315%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.6082%\n",
      "Mean MSE: 0.3642\n",
      "95% Confidence Interval: (0.3574, 0.3711)\n",
      "Margin of Error: 0.0069\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4bde3",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91738062",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': False, 'RADAR': True}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4772ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "269b1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae3c5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3775cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a5ed9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7377a7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52eff20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 71.5562%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.4503%\n",
      "Mean MSE: 0.3211\n",
      "95% Confidence Interval: (0.3160, 0.3261)\n",
      "Margin of Error: 0.0051\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5189101d",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3042c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': False, 'RADAR': False}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b54955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "383e7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7aaac752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f07ceb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c8787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f064fc1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "780b85c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 71.1381%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.4912%\n",
      "Mean MSE: 0.3258\n",
      "95% Confidence Interval: (0.3202, 0.3313)\n",
      "Margin of Error: 0.0055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d54ae8",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df5d467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': True, 'RADAR': True}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d587d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a54246cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a626ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d8c53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f91fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96400c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13325eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.7899%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.5705%\n",
      "Mean MSE: 0.3523\n",
      "95% Confidence Interval: (0.3458, 0.3587)\n",
      "Margin of Error: 0.0064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afdd340",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d628b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': False, 'RADAR': True}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6854cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2781ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20fc5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3cc8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c9936b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a06de",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f4e9969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 66.8164%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.9840%\n",
      "Mean MSE: 0.3746\n",
      "95% Confidence Interval: (0.3634, 0.3857)\n",
      "Margin of Error: 0.0111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eac7cb",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d78c0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': True, 'RADAR': False}\n",
    "weight_path=f'models/CSINetplustask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e56a378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "993897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad14ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task3Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded, _ =self.bde(Hencoded, input_autoregressive_features)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        \n",
    "        autoregressive_features = output\n",
    "        #print(output.shape)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output, autoregressive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be748d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = torch.load(weight_pathb+\"task3Encoder.pth\")\n",
    "        self.de = Decoderwithmsi(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "       \n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, gps, radar, left_cam, center_cam, right_cam, onoffdict, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8774b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e288c4e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f10d2341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.6739%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.5175%\n",
      "Mean MSE: 0.3536\n",
      "95% Confidence Interval: (0.3477, 0.3594)\n",
      "Margin of Error: 0.0058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task3.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c22e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
