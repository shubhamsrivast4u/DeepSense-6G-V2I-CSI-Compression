{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Select the GPU index\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "from scipy.io import savemat\n",
    "from scipy import stats\n",
    "import dill as pickle\n",
    "import thop\n",
    "from torch_challenge_dataset import DeepVerseChallengeLoaderTaskThree\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f11ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "onoffdict={'GPS': False, 'CAMERAS': False, 'RADAR': False}\n",
    "reduction = 4\n",
    "weight_path=f'models/ACRNettask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8236d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a3e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c6170",
   "metadata": {},
   "source": [
    "# Utils and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab275daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_abs_reshape(y, csi_std=2.8117975e-06, target_std=1.0):\n",
    "    y = torch.abs(y)\n",
    "    y=(y/csi_std)*target_std\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73152fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_reshape( y, csi_std=2.5e-06, target_std=1):\n",
    "        ry = torch.real(y)\n",
    "        iy= torch.imag(y)\n",
    "        oy=torch.cat([ry,iy],dim=1)\n",
    "        #scaling\n",
    "        oy=(oy/csi_std)*target_std\n",
    "        return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f12349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b783bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_parameters(model):\n",
    "    total_param  = []\n",
    "    for p1 in model.parameters():\n",
    "        total_param.append(int(p1.numel()))\n",
    "    return sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_channels=in_planes,\n",
    "                               out_channels=out_planes,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               groups=groups,\n",
    "                               bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes))\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACRDecoderBlock(nn.Module):\n",
    "    r\"\"\" Inverted residual with extensible width and group conv\n",
    "    \"\"\"\n",
    "    def __init__(self, expansion=20):\n",
    "        super(ACRDecoderBlock, self).__init__()\n",
    "        width = 8 * expansion\n",
    "        self.conv1_bn = ConvBN(2, width, [1, 9])\n",
    "        self.prelu1 = nn.PReLU(num_parameters=width, init=0.3)\n",
    "        self.conv2_bn = ConvBN(width, width, 7, groups=4 * expansion)\n",
    "        self.prelu2 = nn.PReLU(num_parameters=width, init=0.3)\n",
    "        self.conv3_bn = ConvBN(width, 2, [9, 1])\n",
    "        self.prelu3 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        residual = self.prelu1(self.conv1_bn(x))\n",
    "        residual = self.prelu2(self.conv2_bn(residual))\n",
    "        residual = self.conv3_bn(residual)\n",
    "\n",
    "        return self.prelu3(identity + residual)\n",
    "\n",
    "\n",
    "class ACREncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ACREncoderBlock, self).__init__()\n",
    "        self.conv_bn1 = ConvBN(2, 2, [1, 9])\n",
    "        self.prelu1 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.conv_bn2 = ConvBN(2, 2, [9, 1])\n",
    "        self.prelu2 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        residual = self.prelu1(self.conv_bn1(x))\n",
    "        residual = self.conv_bn2(residual)\n",
    "\n",
    "        return self.prelu2(identity + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f70d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16, expansion=20):\n",
    "        super(task2Encoder, self).__init__()\n",
    "        self.total_size =8192\n",
    "        n1=int(math.log2(reduction))\n",
    "        self.encoder_feature = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(1, 2, 5)),\n",
    "            (\"prelu\", nn.PReLU(num_parameters=2, init=0.3)),\n",
    "            (\"ACREncoderBlock1\", ACREncoderBlock()),\n",
    "            (\"ACREncoderBlock2\", ACREncoderBlock()),\n",
    "        ]))\n",
    "        self.encoder_fc = nn.Linear(self.total_size, self.total_size // reduction)\n",
    "        self.output_sig = nn.Sigmoid()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.detach().size()\n",
    "        \n",
    "        out = self.encoder_feature(x.to(torch.float32))\n",
    "        out =  self.encoder_fc(out.view(n, -1))\n",
    "        \n",
    "        \n",
    "        return out\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf1fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16, expansion=20):\n",
    "        super(task2Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.decoder_fc1 = nn.Linear(self.total_size // reduction, self.total_size)\n",
    "        self.decoder_feature = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 2, 5)),\n",
    "            (\"prelu\", nn.PReLU(num_parameters=2, init=0.3)),\n",
    "            (\"ACRDecoderBlock1\", ACRDecoderBlock(expansion=expansion)),\n",
    "            (\"ACRDecoderBlock2\", ACRDecoderBlock(expansion=expansion)),\n",
    "            (\"sigmoid\", nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        self.decoder_fc2 = nn.Linear(self.total_size, self.total_size//2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, Hencoded):\n",
    "        bs = Hencoded.size(0)\n",
    "        #combining\n",
    "        out = Hencoded.view(bs, self.reduced_size)\n",
    "        # Generate final output\n",
    "        out = self.decoder_fc1(out)\n",
    "        out = self.decoder_feature(out.view(bs, -1, 64, 64))\n",
    "        out = self.sig2(self.decoder_fc2(out.view(bs, -1)))\n",
    "        \n",
    "        return out.view(bs, -1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cbed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ed9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task2weight_path=f'models/ACRNettask2/cr{reduction}/gpsFalse_camFalse_radFalse/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db01c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Encoder, self).__init__()\n",
    "        \n",
    "\n",
    "        #self.en=task2Encoder(reduction)\n",
    "        #reduction value is already considered in the task2weight_path\n",
    "        # loading preloaded values\n",
    "        self.en=torch.load(task2weight_path+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the parameters to match the batch size\n",
    "        if self.allow_update:\n",
    "            out = self.en(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.en(x)\n",
    "        \n",
    "        encoded_features=out\n",
    "        return encoded_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0586ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        self.de = torch.load(task2weight_path+\"task2Decoder.pth\")\n",
    "        \n",
    "        #Layers for auto regression \n",
    "        self.a= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.b= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.c= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.d= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hencoded, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        a = self.a.expand(bs, -1)\n",
    "        b = self.b.expand(bs, -1)\n",
    "        c = self.c.expand(bs, -1)\n",
    "        d = self.d.expand(bs, -1)\n",
    "        out_tminus1=input_autoregressive_features[:,0,:].view(bs,-1)\n",
    "        out_tminus2=input_autoregressive_features[:,1,:].view(bs,-1)\n",
    "        if self.allow_update:\n",
    "            out_t = self.de(Hencoded)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out_t = self.de(Hencoded)\n",
    "        #print(out_t.shape)\n",
    "        out = (out_t.view(bs,-1)) * a + out_tminus1 * b + out_tminus2 * c + d\n",
    "        \n",
    "        autoregressive_features = out\n",
    "        \n",
    "        output = out.view(bs,1, 64, 64)\n",
    "        \n",
    "        return output, autoregressive_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51999f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = task3Encoder(reduction)\n",
    "        self.de = task3Decoder(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, X, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "        Hin = X\n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2ba50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#criterion=nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6c938",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7cd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, device, criterion):\n",
    "    num_test_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse1 = 0\n",
    "        for b,t_x in enumerate(test_loader):\n",
    "            model.ar = [None] * 5 \n",
    "            for time_index,(X, y) in enumerate(t_x):\n",
    "                y_test_reshaped = CSI_abs_reshape(y.to(device))\n",
    "                Xin = CSI_abs_reshape(X[0].to(device))\n",
    "                # Get the input and output for the given time index\n",
    "                y_pred = model(Xin, time_index, device, is_training=True, onoffdict = onoffdict)\n",
    "                mse0 = criterion(y_pred, y_test_reshaped) \n",
    "                mse1+=mse0  \n",
    "        avg_mse=mse1/(5*num_test_batches)\n",
    "    return avg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5399da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DeepVerseChallengeLoaderTaskThree(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = torch.tensor([])\n",
    "for b,t_x in enumerate(test_loader):\n",
    "        for time_index,(X, y) in enumerate(t_x):\n",
    "            h = CSI_abs_reshape(y)\n",
    "            h_list = torch.cat([h_list,h])\n",
    "target_loss = torch.mean((torch.abs(h_list) - torch.mean(torch.abs(h_list))) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0501436",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83fa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 39.8561%\n",
      "Percentage Improvement Confidence Interval Achieved: 1.2695%\n",
      "Mean MSE: 0.6789\n",
      "95% Confidence Interval: (0.6645, 0.6932)\n",
      "Margin of Error: 0.0143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task3.pth\").to(device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180f32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
