{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Select the GPU index\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "import spacy\n",
    "from scipy.io import savemat\n",
    "from scipy import stats\n",
    "import dill as pickle\n",
    "import thop\n",
    "from torch_challenge_dataset import DeepVerseChallengeLoaderTaskTwo\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75a6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "onoffdict={'GPS': True, 'CAMERAS': True, 'RADAR': True}\n",
    "reduction = 4\n",
    "batch_size = 200\n",
    "num_H = 64\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef315b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "036636ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/ACRNettask2withMSI/cr4/gpsTrue_camTrue_radTrue/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dae38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a3e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce5db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DeepVerseChallengeLoaderTaskTwo(csv_path = r'./dataset_train.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, num_workers=4)\n",
    "test_dataset = DeepVerseChallengeLoaderTaskTwo(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c6170",
   "metadata": {},
   "source": [
    "# Utils and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab275daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_abs_reshape(y, csi_std=2.8117975e-06, target_std=1.0):\n",
    "    y = torch.abs(y)\n",
    "    y=(y/csi_std)*target_std\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e01a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_reshape( y, csi_std=2.5e-06, target_std=1):\n",
    "        ry = torch.real(y)\n",
    "        iy= torch.imag(y)\n",
    "        oy=torch.cat([ry,iy],dim=1)\n",
    "        #scaling\n",
    "        oy=(oy/csi_std)*target_std\n",
    "        return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b783bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_parameters(model):\n",
    "    total_param  = []\n",
    "    for p1 in model.parameters():\n",
    "        total_param.append(int(p1.numel()))\n",
    "    return sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee9b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    # Convert image to float tensor\n",
    "    image = image.float()\n",
    "    # Normalize the image\n",
    "    image /= 255.0\n",
    "    # ImageNet mean values # ImageNet standard deviation values\n",
    "    trans=T.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) \n",
    "    image=trans(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2581a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_coordinates_batch(x_cor, y_cor):\n",
    "    y_pix = torch.zeros_like(x_cor)\n",
    "    x_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition1 = y_cor < -4\n",
    "    condition2 = (y_cor >= -4) & (y_cor < -1)\n",
    "    condition3 = (y_cor >= -1) & (y_cor < 1)\n",
    "    condition4 = (y_cor >= 1) & (y_cor < 4)\n",
    "    condition5 = y_cor >= 4\n",
    "\n",
    "    y_pix[condition1] = 100 + (250 - 100) * ((x_cor[condition1] - 80) / (125 - 80))\n",
    "    y_pix[condition2] = 100 + (250 - 100) * ((x_cor[condition2] - 80) / (125 - 80))\n",
    "    y_pix[condition3] = 100 + (250 - 100) * ((x_cor[condition3] - 80) / (125 - 80))\n",
    "    y_pix[condition4] = 100 + (210 - 100) * ((x_cor[condition4] - 80) / (125 - 80))\n",
    "    y_pix[condition5] = 100 + (190 - 100) * ((x_cor[condition5] - 80) / (125 - 80))\n",
    "\n",
    "    x_pix[condition1] = (y_pix[condition1] - 30) / 1.35\n",
    "    x_pix[condition2] = (y_pix[condition2] - 45) / 0.85\n",
    "    x_pix[condition3] = (y_pix[condition3] - 55) / 0.70\n",
    "    x_pix[condition4] = (y_pix[condition4] - 65) / 0.60\n",
    "    x_pix[condition5] = (y_pix[condition5] - 65) / 0.5\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c4777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_coordinates_batch(x_cor, y_cor):\n",
    "    x_pix = torch.zeros_like(x_cor)\n",
    "    y_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition = y_cor < 0\n",
    "    x_pix[condition] = 256 * ((x_cor[condition] - 119) / (139 - 119))\n",
    "    x_pix[~condition] = 256 * ((x_cor[~condition] - 112) / (146 - 113))\n",
    "    \n",
    "    y_pix = 175 + (100 - 175) * ((y_cor - (-7)) / ((7) - (-7)))\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce57e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_coordinates_batch(x_cor, y_cor):\n",
    "    y_pix = torch.zeros_like(x_cor)\n",
    "    x_pix = torch.zeros_like(y_cor)\n",
    "\n",
    "    condition1 = y_cor < -4\n",
    "    condition2 = (y_cor >= -4) & (y_cor < -1)\n",
    "    condition3 = (y_cor >= -1) & (y_cor < 1)\n",
    "    condition4 = (y_cor >= 1) & (y_cor < 4)\n",
    "    condition5 = y_cor >= 4\n",
    "\n",
    "    y_pix[condition1] = 250 + (100 - 250) * ((x_cor[condition1] - 125) / (200 - 125))\n",
    "    y_pix[condition2] = 250 + (100 - 250) * ((x_cor[condition2] - 125) / (200 - 125))\n",
    "    y_pix[condition3] = 250 + (100 - 250) * ((x_cor[condition3] - 125) / (200 - 125))\n",
    "    y_pix[condition4] = 210 + (100 - 210) * ((x_cor[condition4] - 125) / (200 - 125))\n",
    "    y_pix[condition5] = 190 + (100 - 190) * ((x_cor[condition5] - 125) / (200 - 125))\n",
    "\n",
    "    x_pix[condition1] = -(y_pix[condition1] - 370) / 1.25\n",
    "    x_pix[condition2] = -(y_pix[condition2] - 285) / 0.87\n",
    "    x_pix[condition3] = -(y_pix[condition3] - 250) / 0.73\n",
    "    x_pix[condition4] = -(y_pix[condition4] - 210) / 0.55\n",
    "    x_pix[condition5] = -(y_pix[condition5] - 190) / 0.45\n",
    "    return x_pix, y_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c98e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_image_batch(images, center_x, center_y, output_size, bounded=False):\n",
    "    batch_size = images.size(0)\n",
    "    \n",
    "    if bounded == 'left':\n",
    "        top = torch.clamp(center_y - output_size[0] // 2, 0, None)\n",
    "        left = torch.clamp(center_x - output_size[1] // 2, 0, None)\n",
    "    elif bounded == 'right':\n",
    "        bottom = center_y + output_size[0] // 2\n",
    "        right = torch.clamp(center_x + output_size[1] // 2, None, 250)\n",
    "        top = torch.clamp(bottom - output_size[0], 0, None)\n",
    "        left = right - output_size[1]\n",
    "    else:\n",
    "        top = center_y - output_size[0] // 2\n",
    "        left = center_x - output_size[1] // 2\n",
    "\n",
    "    resize_transform = transforms.Resize((output_size))\n",
    "    cropped_images = [TF.crop(image, int(top[i].item()), int(left[i].item()), output_size[0], output_size[1]) \n",
    "                        for i, image in enumerate(images)]\n",
    "    cropped_images = torch.stack([resize_transform(image) for image in cropped_images])\n",
    "    return cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a93af7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_imgs(gps, img_1, img_2, img_3, crop_size = (150,150)):\n",
    "    x_cor = gps[:,0]\n",
    "    y_cor = gps[:,1]\n",
    "\n",
    "    x_pix,y_pix = left_coordinates_batch(x_cor, y_cor)\n",
    "    img_1 = center_image_batch(img_1, x_pix.to(torch.int), y_pix.to(torch.int), crop_size, 'left')\n",
    "\n",
    "    x_pix,y_pix = center_coordinates_batch(x_cor, y_cor)\n",
    "    img_2 = center_image_batch(img_2, x_pix.to(torch.int), y_pix.to(torch.int), crop_size)\n",
    "\n",
    "    x_pix,y_pix = right_coordinates_batch(x_cor, y_cor)\n",
    "    img_3 = center_image_batch(img_3, x_pix.to(torch.int), y_pix.to(torch.int), crop_size, 'right')\n",
    "\n",
    "    return img_1, img_2, img_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516ccee",
   "metadata": {},
   "source": [
    "Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25de8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpsdata(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gps_fc = nn.Linear(2, 16)\n",
    "        self.gps_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, gps):  \n",
    "        gps = gps.to(torch.float32)\n",
    "\n",
    "        x, y = gps[:,0], gps[:,1]\n",
    "        x_normd = (x - torch.min(x)) / (torch.max(x) - torch.min(x))\n",
    "        y_normd = (y - torch.min(y)) / (torch.max(y) - torch.min(y))\n",
    "        gps_normd = torch.stack([x_normd,y_normd],dim=1)\n",
    "\n",
    "        gps_out = self.gps_fc(gps_normd)  \n",
    "        gps_out = self.gps_relu(gps_out)\n",
    "        return gps_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a26d3e",
   "metadata": {},
   "source": [
    "Radar Data Processing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e65f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class radardata(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(radardata, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 1, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.lr1=nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        self.encoder_fc = nn.Linear(256,16)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,1,512,128)\n",
    "        x = (x - 5.1838e-06) / (28.0494 - 5.1838e-06)\n",
    "        out = self.pool1(self.dropout(self.conv1(x)))\n",
    "        out = self.pool2(self.dropout(self.conv2(out))).view(x.size(0), -1)\n",
    "        out = self.dropout(self.encoder_fc(out))\n",
    "        out = self.lr1(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004a71e",
   "metadata": {},
   "source": [
    "Camera Data Processing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dce3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cameradata(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 1, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.lr1=nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        self.encoder = nn.Linear(1*81,16)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, cam):  \n",
    "        cam = normalize_image(cam).to(torch.float32)\n",
    "        out = self.pool1(self.dropout(self.conv1(cam)))\n",
    "        out = self.pool2(self.dropout(self.conv2(out)))\n",
    "        out = self.lr1(out).view(-1,1*81)\n",
    "        out = self.dropout(self.encoder(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c21d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_channels=in_planes,\n",
    "                               out_channels=out_planes,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding,\n",
    "                               groups=groups,\n",
    "                               bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes))\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89e3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACRDecoderBlock(nn.Module):\n",
    "    r\"\"\" Inverted residual with extensible width and group conv\n",
    "    \"\"\"\n",
    "    def __init__(self, expansion):\n",
    "        super(ACRDecoderBlock, self).__init__()\n",
    "        width = 8 * expansion\n",
    "        self.conv1_bn = ConvBN(2, width, [1, 9])\n",
    "        self.prelu1 = nn.PReLU(num_parameters=width, init=0.3)\n",
    "        self.conv2_bn = ConvBN(width, width, 7, groups=4 * expansion)\n",
    "        self.prelu2 = nn.PReLU(num_parameters=width, init=0.3)\n",
    "        self.conv3_bn = ConvBN(width, 2, [9, 1])\n",
    "        self.prelu3 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        residual = self.prelu1(self.conv1_bn(x))\n",
    "        residual = self.prelu2(self.conv2_bn(residual))\n",
    "        residual = self.conv3_bn(residual)\n",
    "\n",
    "        return self.prelu3(identity + residual)\n",
    "\n",
    "\n",
    "class ACREncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ACREncoderBlock, self).__init__()\n",
    "        self.conv_bn1 = ConvBN(2, 2, [1, 9])\n",
    "        self.prelu1 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.conv_bn2 = ConvBN(2, 2, [9, 1])\n",
    "        self.prelu2 = nn.PReLU(num_parameters=2, init=0.3)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        residual = self.prelu1(self.conv_bn1(x))\n",
    "        residual = self.conv_bn2(residual)\n",
    "\n",
    "        return self.prelu2(identity + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2f70d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16, expansion=20):\n",
    "        super(task2Encoder, self).__init__()\n",
    "        self.total_size =8192\n",
    "        n1=int(math.log2(reduction))\n",
    "        self.encoder_feature = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(1, 2, 5)),\n",
    "            (\"prelu\", nn.PReLU(num_parameters=2, init=0.3)),\n",
    "            (\"ACREncoderBlock1\", ACREncoderBlock()),\n",
    "            (\"ACREncoderBlock2\", ACREncoderBlock()),\n",
    "        ]))\n",
    "        self.encoder_fc = nn.Linear(self.total_size, self.total_size // reduction)\n",
    "        self.output_sig = nn.Sigmoid()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.detach().size()\n",
    "        \n",
    "        out = self.encoder_feature(x.to(torch.float32))\n",
    "        out =  self.encoder_fc(out.view(n, -1))\n",
    "        \n",
    "        \n",
    "        return out\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf1fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16, expansion=20):\n",
    "        super(task2Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.decoder_fc1 = nn.Linear(self.total_size // reduction, self.total_size)\n",
    "        self.decoder_feature = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 2, 5)),\n",
    "            (\"prelu\", nn.PReLU(num_parameters=2, init=0.3)),\n",
    "            (\"ACRDecoderBlock1\", ACRDecoderBlock(expansion=expansion)),\n",
    "            (\"ACRDecoderBlock2\", ACRDecoderBlock(expansion=expansion)),\n",
    "            (\"sigmoid\", nn.Sigmoid())\n",
    "        ]))\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        self.decoder_fc2 = nn.Linear(self.total_size, self.total_size//2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, Hencoded):\n",
    "        bs = Hencoded.size(0)\n",
    "        #combining\n",
    "        out = Hencoded.view(bs, self.reduced_size)\n",
    "        # Generate final output\n",
    "        out = self.decoder_fc1(out)\n",
    "        out = self.decoder_feature(out.view(bs, -1, 64, 64))\n",
    "        out = self.sig2(self.decoder_fc2(out.view(bs, -1)))\n",
    "        \n",
    "        return out.view(bs, -1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ecab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd4bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading weights of baseline model and task 1\n",
    "onoffdictb={'GPS': False, 'CAMERAS': False, 'RADAR': False} #baseline dictionary\n",
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'\n",
    "weight_pathb=f'models/ACRNettask2/cr{reduction}/gps{onoffdictb[\"GPS\"]}_cam{onoffdictb[\"CAMERAS\"]}_rad{onoffdictb[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14888412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task1decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gp = gpsdata()\n",
    "        self.rd = radardata()\n",
    "        self.lc = cameradata()\n",
    "        self.cc = cameradata()\n",
    "        self.rc = cameradata()\n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = nn.Linear(16*5, int(num_H/2)*int(num_H/2))\n",
    "            self.output_fc = nn.Linear(int(num_H/2)*int(num_H/2), num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = nn.Linear(16*5, 32)\n",
    "            self.output_fc = nn.Linear(32, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = gps.size(0)\n",
    "        \n",
    "        if onoffdict['GPS']:\n",
    "             gps_out = self.gp(gps)\n",
    "        else:\n",
    "             gps_out = torch.zeros(bs, 16).to(device)\n",
    "        \n",
    "        if onoffdict['RADAR']:\n",
    "            radar_out = self.rd(radar)\n",
    "        else:\n",
    "            radar_out = torch.zeros(bs, 16).to(device)\n",
    "        \n",
    "        if onoffdict['CAMERAS']:\n",
    "            left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "            lc_out = self.lc(left_cam)\n",
    "            cc_out = self.cc(center_cam)\n",
    "            rc_out = self.rc(right_cam)\n",
    "        else:\n",
    "            lc_out = torch.zeros(bs, 16).to(device)\n",
    "            cc_out = torch.zeros(bs, 16).to(device)\n",
    "            rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "        combined = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "        \n",
    "        output = self.linear(combined)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc(output)\n",
    "        output = self.output_relu(output)\n",
    "        output = output.view(output.size(0), 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24be9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8378ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2ba50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#criterion=nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6c938",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09917417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, device, criterion):\n",
    "    num_test_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse1 = 0\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            y_test = y_test.to(device)\n",
    "            Xin = CSI_abs_reshape(X_test[0])\n",
    "            y_pred = model(Xin.to(device),X_test[1].to(device),X_test[2].to(device),X_test[3].to(device),X_test[4].to(device),X_test[5].to(device), onoffdict = onoffdict)\n",
    "            y_test_reshaped = CSI_abs_reshape(y_test)\n",
    "            mse0 = criterion(y_pred, y_test_reshaped) \n",
    "            mse1 += mse0 \n",
    "        \n",
    "    avg_mse = mse1 / num_test_batches\n",
    "    return avg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "910180db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6b9decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DeepVerseChallengeLoaderTaskTwo(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70a7d594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "h_list = torch.tensor([])\n",
    "for b, (x,h) in enumerate(test_loader):\n",
    "    h = CSI_abs_reshape(h)\n",
    "    h_list = torch.cat([h_list,h])\n",
    "target_loss = torch.mean((torch.abs(h_list) - torch.mean(torch.abs(h_list))) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11097b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b69466f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.5933%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.3452%\n",
      "Mean MSE: 0.3504\n",
      "95% Confidence Interval: (0.3466, 0.3543)\n",
      "Margin of Error: 0.0039\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19646553",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d02138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': True, 'RADAR': False}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd977a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc2ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f43ca2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fe56827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58d857b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0d773",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4db6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.0972%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.5037%\n",
      "Mean MSE: 0.3560\n",
      "95% Confidence Interval: (0.3503, 0.3616)\n",
      "Margin of Error: 0.0056\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43022b",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0908d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': False, 'RADAR': True}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c1182d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d206c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e2782ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6aa587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef64e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d649d7",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cc775e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.4474%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.4046%\n",
      "Mean MSE: 0.3521\n",
      "95% Confidence Interval: (0.3475, 0.3566)\n",
      "Margin of Error: 0.0045\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae1529",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2f72743",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': True, 'CAMERAS': False, 'RADAR': False}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02e87ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb8b49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce1cefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e685068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66cfe4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efa9a6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5c88883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 68.8519%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.4120%\n",
      "Mean MSE: 0.3475\n",
      "95% Confidence Interval: (0.3429, 0.3521)\n",
      "Margin of Error: 0.0046\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b91f7e",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d81898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': True, 'RADAR': True}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b66eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c77e6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d0efe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf923e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8eed0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b824d73",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3c2a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 69.3302%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.3162%\n",
      "Mean MSE: 0.3422\n",
      "95% Confidence Interval: (0.3387, 0.3457)\n",
      "Margin of Error: 0.0035\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3297f",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13948651",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': True, 'RADAR': False}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d041e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d44e3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03d2bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c1ed6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eca2aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cbfba",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "707e100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 69.1367%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.2237%\n",
      "Mean MSE: 0.3444\n",
      "95% Confidence Interval: (0.3419, 0.3469)\n",
      "Margin of Error: 0.0025\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b94460",
   "metadata": {},
   "source": [
    "# Change Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50ee2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "onoffdict={'GPS': False, 'CAMERAS': False, 'RADAR': True}\n",
    "weight_path=f'models/ACRNettask2withMSI/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4fa7af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59e606cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_weight_path=f'models/task1/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eddbcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderwithmsi(nn.Module):\n",
    "    def __init__(self, reduction):\n",
    "        super().__init__()\n",
    "        self.task1decoder = torch.load(task1_weight_path+\"task1Decoder.pth\")\n",
    "        self.gp = self.task1decoder.gp\n",
    "        self.rd = self.task1decoder.rd\n",
    "        self.lc = self.task1decoder.lc\n",
    "        self.cc = self.task1decoder.cc\n",
    "        self.rc = self.task1decoder.rc\n",
    "        self.bde = torch.load(weight_pathb+\"task2Decoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        #self.bde = baselinedecoder(reduction) \n",
    "\n",
    "        if int(num_H/2)*int(num_H/2) > 32:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1= nn.Linear(int(num_H/2)*int(num_H/2)+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        else:\n",
    "            self.linear = self.task1decoder.linear\n",
    "            self.output_fc1 = nn.Linear(32+num_H*num_H, 2*num_H*num_H)\n",
    "            self.output_fc2 = nn.Linear(2*num_H*num_H, num_H*num_H)\n",
    "            self.output_relu = nn.ReLU()\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict):\n",
    "        bs = Hencoded.size(0)\n",
    "        if self.allow_update:\n",
    "            Hdecoded=self.bde(Hencoded)\n",
    "            if onoffdict['GPS']:\n",
    "                 gps_out = self.gp(gps)\n",
    "            else:\n",
    "                 gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['RADAR']:\n",
    "                radar_out = self.rd(radar)\n",
    "            else:\n",
    "                radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            if onoffdict['CAMERAS']:\n",
    "                left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                lc_out = self.lc(left_cam)\n",
    "                cc_out = self.cc(center_cam)\n",
    "                rc_out = self.rc(right_cam)\n",
    "            else:\n",
    "                lc_out = torch.zeros(bs, 16).to(device)\n",
    "                cc_out = torch.zeros(bs, 16).to(device)\n",
    "                rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "            combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "            output = self.linear(combined1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hdecoded=self.bde(Hencoded)\n",
    "                if onoffdict['GPS']:\n",
    "                     gps_out = self.gp(gps)\n",
    "                else:\n",
    "                     gps_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['RADAR']:\n",
    "                    radar_out = self.rd(radar)\n",
    "                else:\n",
    "                    radar_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                if onoffdict['CAMERAS']:\n",
    "                    left_cam, center_cam, right_cam = process_imgs(gps, left_cam, center_cam, right_cam, crop_size = (150,150))\n",
    "                    lc_out = self.lc(left_cam)\n",
    "                    cc_out = self.cc(center_cam)\n",
    "                    rc_out = self.rc(right_cam)\n",
    "                else:\n",
    "                    lc_out = torch.zeros(bs, 16).to(device)\n",
    "                    cc_out = torch.zeros(bs, 16).to(device)\n",
    "                    rc_out = torch.zeros(bs, 16).to(device)\n",
    "\n",
    "                combined1 = torch.cat((gps_out, radar_out, lc_out, cc_out, rc_out), dim=1)\n",
    "\n",
    "                output = self.linear(combined1)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        output = self.output_relu(output)\n",
    "        combined2 = torch.cat((output, Hdecoded.view(bs,-1)), dim=1)\n",
    "        output = self.output_fc1(combined2)\n",
    "        output = self.output_relu(output)\n",
    "        output = self.output_fc2(output)\n",
    "        output = output.view(bs, 1, num_H, num_H)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13bad0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=torch.load(weight_pathb+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "        self.de=Decoderwithmsi(reduction)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, gps, radar, left_cam, center_cam, right_cam, onoffdict): \n",
    "        \n",
    "        #Encoder\n",
    "        if self.allow_update:\n",
    "            Hencoded=self.en(Hin)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        Hdecoded=self.de(Hencoded,gps,radar,left_cam,center_cam,right_cam,onoffdict)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "447150b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task2model(reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4deac",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9049c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 69.8923%\n",
      "Percentage Improvement Confidence Interval Achieved: 0.2291%\n",
      "Mean MSE: 0.3359\n",
      "95% Confidence Interval: (0.3334, 0.3385)\n",
      "Margin of Error: 0.0026\n"
     ]
    }
   ],
   "source": [
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task2.pth\", map_location=device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b61a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
