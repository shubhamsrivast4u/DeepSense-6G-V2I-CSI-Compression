{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Select the GPU index\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "import spacy\n",
    "from scipy.io import savemat\n",
    "from scipy import stats\n",
    "import dill as pickle\n",
    "import thop\n",
    "from typing import Optional, Tuple, Any\n",
    "from typing import List, Optional, Tuple\n",
    "from torch_challenge_dataset import DeepVerseChallengeLoaderTaskTwo\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bfc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "onoffdict={'GPS': False, 'CAMERAS': False, 'RADAR': False}\n",
    "reduction = 8\n",
    "batch_size = 200\n",
    "weight_path=f'models/TransNettask2/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef315b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036636ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/TransNettask2/cr8/gpsFalse_camFalse_radFalse/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dae38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a3e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca6927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78c6170",
   "metadata": {},
   "source": [
    "# Utils and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e01a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_reshape( y, csi_std=2.5e-06, target_std=1):\n",
    "        ry = torch.real(y)\n",
    "        iy= torch.imag(y)\n",
    "        oy=torch.cat([ry,iy],dim=1)\n",
    "        #scaling\n",
    "        oy=(oy/csi_std)*target_std\n",
    "        return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab275daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_abs_reshape(y, csi_std=2.8117975e-06, target_std=1.0):\n",
    "    y = torch.abs(y)\n",
    "    y=(y/csi_std)*target_std\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60031c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing output as neural network doesnot understand complex numbers, without normalization\n",
    "def CSI_complex2real(y):\n",
    "    ry = torch.real(y)\n",
    "    iy= torch.imag(y)\n",
    "    oy=torch.cat([ry,iy],dim=1)\n",
    "    return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b783bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_parameters(model):\n",
    "    total_param  = []\n",
    "    for p1 in model.parameters():\n",
    "        total_param.append(int(p1.numel()))\n",
    "    return sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dot_attention(\n",
    "       q:Tensor,\n",
    "       k:Tensor,\n",
    "       v:Tensor,\n",
    "       dropout_p:float = 0.0,\n",
    "       attn_mask : Optional[Tensor] = None,\n",
    ")-> Tuple[Tensor,Tensor]:\n",
    "    \n",
    "    _,_,E = q.shape\n",
    "    q = q / math.sqrt(E)\n",
    "    attn = torch.bmm(q,k.transpose(-2,-1))\n",
    "    if attn_mask is not None:\n",
    "        attn = attn + attn_mask\n",
    "    attn = F.softmax(attn,dim =-1)\n",
    "    if dropout_p:\n",
    "        attn = F.dropout(attn,p = dropout_p)\n",
    "    out = torch.bmm(attn,v)\n",
    "\n",
    "    return out,attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_forward(\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        num_heads: int,\n",
    "        in_proj_weight: Tensor,\n",
    "        in_proj_bias: Optional[Tensor],\n",
    "        dropout_p: float,\n",
    "        out_proj_weight: Tensor,\n",
    "        out_proj_bias: Optional[Tensor],\n",
    "        training: bool = True,\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        need_weights: bool = True,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        use_separate_proj_weight=None,\n",
    "        q_proj_weight: Optional[Tensor] = None,\n",
    "        k_proj_weight: Optional[Tensor] = None,\n",
    "        v_proj_weight: Optional[Tensor] = None,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \n",
    "    tgt_len, bsz, embed_dim = query.shape\n",
    "    src_len, _, _ = key.shape\n",
    "    head_dim = embed_dim // num_heads\n",
    "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
    "    \n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "        else:\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f\"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}\"\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\n",
    "            \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    \n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (bsz, src_len), \\\n",
    "            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
    "        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len). \\\n",
    "            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
    "        if attn_mask is None:\n",
    "            attn_mask = key_padding_mask\n",
    "        elif attn_mask.dtype == torch.bool:\n",
    "            attn_mask = attn_mask.logical_or(key_padding_mask)\n",
    "        else:\n",
    "            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
    "  \n",
    "    if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
    "        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n",
    "        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "        attn_mask = new_attn_mask\n",
    "\n",
    "    \n",
    "    if not training:\n",
    "        dropout_p = 0.0\n",
    "    attn_output, attn_output_weights = scale_dot_attention(q, k, v, attn_mask, dropout_p)\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1518927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _in_projection_packed(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    ") -> List[Tensor]:\n",
    "    E = q.size(-1)\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            return F.linear(q, w, b).chunk(3, dim=-1)\n",
    "        else:\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcdf9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True,\n",
    "                 kdim=None, vdim=None, batch_first=False) -> None:\n",
    "        # factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == self.embed_dim and self.vdim == self.embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim)))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim)))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim)))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim,embed_dim)))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        if self.batch_first:\n",
    "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                query, key, value, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                query, key, value, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)\n",
    "            \n",
    "        if self.batch_first:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033e780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, batch_first=False) -> None:\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model,nhead,\n",
    "                                            dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a419510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layer = encoder_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for _ in range(self.num_layers):\n",
    "            output = self.layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec5e8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Layer:\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, batch_first=False) -> None:\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model,nhead,\n",
    "                                            dropout=dropout, batch_first=batch_first)\n",
    "        self.multihead_attn = MultiheadAttention(d_model,nhead,dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9146d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layer = decoder_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for _ in range(self.num_layers):\n",
    "            output = self.layer(output, memory, tgt_mask=tgt_mask,\n",
    "                                memory_mask=memory_mask,\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=memory_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9156eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625ab39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f70d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,  d_model: int = 64, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation = F.relu, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False,  reduction=64) -> None:\n",
    "        super(task2Encoder, self).__init__()\n",
    "        self.total_size =8192\n",
    "       \n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first)\n",
    "            encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "       \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert not (self.total_size % self.d_model), 'd_model needs to be divisible by the size of the entire csi matrix (2048)'\n",
    "        self.feature_shape = (self.total_size//(2*self.d_model), self.d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.fc_encoder = nn.Linear(self.total_size//2,self.total_size//reduction)\n",
    "        #self.fc_decoder = nn.Linear(self.total_size//reduction,self.total_size)\n",
    "        self._reset_parameters()\n",
    "\n",
    "                \n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Optional[Tensor]=None, src_mask: Optional[Tensor] = None,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \n",
    "        memory = self.encoder(src.view(-1, self.feature_shape[0], self.feature_shape[1]), mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        memory_encoder = self.fc_encoder(memory.view(memory.shape[0],-1))\n",
    "        \n",
    "        return memory_encoder\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "   \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf1fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,  d_model: int = 64, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation = F.relu, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False,  reduction=64) -> None:\n",
    "        super(task2Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first)\n",
    "            decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert not (self.total_size % self.d_model), 'd_model needs to be divisible by the size of the entire csi matrix (2048)'\n",
    "        self.feature_shape = (self.total_size//self.d_model, self.d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        #self.fc_encoder = nn.Linear(self.total_size,self.total_size//reduction)\n",
    "        self.fc_decoder = nn.Linear(self.total_size//reduction,self.total_size)\n",
    "        self._reset_parameters()\n",
    "        \n",
    "        self.decoder_fc2 = nn.Linear(self.total_size, self.total_size//2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, memory_encoder, tgt: Optional[Tensor]=None, src_mask: Optional[Tensor] = None,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        bs = memory_encoder.size(0)\n",
    "        \n",
    "        # Generate final output\n",
    "        memory_decoder = self.fc_decoder(memory_encoder).view(-1, self.feature_shape[0], self.feature_shape[1])\n",
    "        output = self.decoder(memory_decoder, memory_decoder, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        output = self.sig2(self.decoder_fc2(output.view(bs, -1)))\n",
    "        \n",
    "        return output.view(bs, -1, 64, 64)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "   \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46df19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=task2Encoder(d_model= 32, num_encoder_layers=2, num_decoder_layers=2, nhead=2, reduction =reduction, dropout= 0.)\n",
    "        \n",
    "        self.de=task2Decoder(d_model=32, num_encoder_layers=2, num_decoder_layers=2, nhead=2, reduction =reduction, dropout= 0.)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, device, is_training): \n",
    "        \n",
    "        #Encoder\n",
    "        Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        #Decoder   \n",
    "        Hdecoded=self.de(Hencoded)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8378ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2ba50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#criterion=nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6c938",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b0c5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, device, criterion):\n",
    "    num_test_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse1 = 0\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            y_test = y_test.to(device)\n",
    "            Xin = CSI_abs_reshape(X_test[0])\n",
    "            y_pred = model(Xin.to(device), device, is_training=False)\n",
    "            y_test_reshaped = CSI_abs_reshape(y_test)\n",
    "            mse0 = criterion(y_pred, y_test_reshaped) \n",
    "            mse1 += mse0 \n",
    "        \n",
    "    avg_mse = mse1 / num_test_batches\n",
    "    return avg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a286d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b8c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DeepVerseChallengeLoaderTaskTwo(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a7d594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "h_list = torch.tensor([])\n",
    "for b, (x,h) in enumerate(test_loader):\n",
    "    h = CSI_abs_reshape(h)\n",
    "    h_list = torch.cat([h_list,h])\n",
    "target_loss = torch.mean((torch.abs(h_list) - torch.mean(torch.abs(h_list))) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef8d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4620bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 17.2390%\n",
      "Percentage Improvement Confidence Interval Achieved: 1.2302%\n",
      "Mean MSE: 0.9234\n",
      "95% Confidence Interval: (0.9097, 0.9372)\n",
      "Margin of Error: 0.0137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    model = torch.load(weight_path + \"task2.pth\").to(device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba981d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
