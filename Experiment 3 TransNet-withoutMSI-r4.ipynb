{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Select the GPU index\n",
    "import scipy.io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import warnings\n",
    "import spacy\n",
    "from scipy.io import savemat\n",
    "from scipy import stats\n",
    "import dill as pickle\n",
    "import thop\n",
    "from typing import Optional, Tuple, Any\n",
    "from typing import List, Optional, Tuple\n",
    "from torch_challenge_dataset import DeepVerseChallengeLoaderTaskThree\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcf443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f11ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "onoffdict={'GPS': False, 'CAMERAS': False, 'RADAR': False}\n",
    "reduction = 4\n",
    "weight_path=f'models/TransNettask3/cr{reduction}/gps{onoffdict[\"GPS\"]}_cam{onoffdict[\"CAMERAS\"]}_rad{onoffdict[\"RADAR\"]}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(weight_path):\n",
    "    os.makedirs(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8236d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce5db81",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Task 3\n",
    "train_dataset = DeepVerseChallengeLoaderTaskThree(csv_path = r'./dataset_train.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataset = DeepVerseChallengeLoaderTaskThree(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a3e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c6170",
   "metadata": {},
   "source": [
    "# Utils and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab275daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_abs_reshape(y, csi_std=2.8117975e-06, target_std=1.0):\n",
    "    y = torch.abs(y)\n",
    "    y=(y/csi_std)*target_std\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73152fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSI_reshape( y, csi_std=2.5e-06, target_std=1):\n",
    "        ry = torch.real(y)\n",
    "        iy= torch.imag(y)\n",
    "        oy=torch.cat([ry,iy],dim=1)\n",
    "        #scaling\n",
    "        oy=(oy/csi_std)*target_std\n",
    "        return oy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe4ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b783bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_model_parameters(model):\n",
    "    total_param  = []\n",
    "    for p1 in model.parameters():\n",
    "        total_param.append(int(p1.numel()))\n",
    "    return sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dot_attention(\n",
    "       q:Tensor,\n",
    "       k:Tensor,\n",
    "       v:Tensor,\n",
    "       dropout_p:float = 0.0,\n",
    "       attn_mask : Optional[Tensor] = None,\n",
    ")-> Tuple[Tensor,Tensor]:\n",
    "    \n",
    "    _,_,E = q.shape\n",
    "    q = q / math.sqrt(E)\n",
    "    attn = torch.bmm(q,k.transpose(-2,-1))\n",
    "    if attn_mask is not None:\n",
    "        attn = attn + attn_mask\n",
    "    attn = F.softmax(attn,dim =-1)\n",
    "    if dropout_p:\n",
    "        attn = F.dropout(attn,p = dropout_p)\n",
    "    out = torch.bmm(attn,v)\n",
    "\n",
    "    return out,attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_forward(\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        num_heads: int,\n",
    "        in_proj_weight: Tensor,\n",
    "        in_proj_bias: Optional[Tensor],\n",
    "        dropout_p: float,\n",
    "        out_proj_weight: Tensor,\n",
    "        out_proj_bias: Optional[Tensor],\n",
    "        training: bool = True,\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        need_weights: bool = True,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        use_separate_proj_weight=None,\n",
    "        q_proj_weight: Optional[Tensor] = None,\n",
    "        k_proj_weight: Optional[Tensor] = None,\n",
    "        v_proj_weight: Optional[Tensor] = None,\n",
    ") -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    \n",
    "    tgt_len, bsz, embed_dim = query.shape\n",
    "    src_len, _, _ = key.shape\n",
    "    head_dim = embed_dim // num_heads\n",
    "    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
    "    \n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "        else:\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f\"Only float, byte, and bool types are supported for attn_mask, not {attn_mask.dtype}\"\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            correct_2d_size = (tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_2d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\")\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "        elif attn_mask.dim() == 3:\n",
    "            correct_3d_size = (bsz * num_heads, tgt_len, src_len)\n",
    "            if attn_mask.shape != correct_3d_size:\n",
    "                raise RuntimeError(\n",
    "                    f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\n",
    "            \"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    \n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.shape == (bsz, src_len), \\\n",
    "            f\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\n",
    "        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len). \\\n",
    "            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)\n",
    "        if attn_mask is None:\n",
    "            attn_mask = key_padding_mask\n",
    "        elif attn_mask.dtype == torch.bool:\n",
    "            attn_mask = attn_mask.logical_or(key_padding_mask)\n",
    "        else:\n",
    "            attn_mask = attn_mask.masked_fill(key_padding_mask, float(\"-inf\"))\n",
    "  \n",
    "    if attn_mask is not None and attn_mask.dtype == torch.bool:\n",
    "        new_attn_mask = torch.zeros_like(attn_mask, dtype=torch.float)\n",
    "        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "        attn_mask = new_attn_mask\n",
    "\n",
    "    \n",
    "    if not training:\n",
    "        dropout_p = 0.0\n",
    "    attn_output, attn_output_weights = scale_dot_attention(q, k, v, attn_mask, dropout_p)\n",
    "    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = nn.functional.linear(attn_output, out_proj_weight, out_proj_bias)\n",
    "    if need_weights:\n",
    "        # average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1518927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _in_projection_packed(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    w: Tensor,\n",
    "    b: Optional[Tensor] = None,\n",
    ") -> List[Tensor]:\n",
    "    E = q.size(-1)\n",
    "    if k is v:\n",
    "        if q is k:\n",
    "            return F.linear(q, w, b).chunk(3, dim=-1)\n",
    "        else:\n",
    "            w_q, w_kv = w.split([E, E * 2])\n",
    "            if b is None:\n",
    "                b_q = b_kv = None\n",
    "            else:\n",
    "                b_q, b_kv = b.split([E, E * 2])\n",
    "            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n",
    "    else:\n",
    "        w_q, w_k, w_v = w.chunk(3)\n",
    "        if b is None:\n",
    "            b_q = b_k = b_v = None\n",
    "        else:\n",
    "            b_q, b_k, b_v = b.chunk(3)\n",
    "        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdf9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True,\n",
    "                 kdim=None, vdim=None, batch_first=False) -> None:\n",
    "        # factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == self.embed_dim and self.vdim == self.embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim)))\n",
    "            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim)))\n",
    "            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim)))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "        else:\n",
    "            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim,embed_dim)))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        if self.batch_first:\n",
    "            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
    "\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                query, key, value, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\n",
    "                v_proj_weight=self.v_proj_weight)\n",
    "        else:\n",
    "            attn_output, attn_output_weights = multi_head_attention_forward(\n",
    "                query, key, value, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)\n",
    "            \n",
    "        if self.batch_first:\n",
    "            return attn_output.transpose(1, 0), attn_output_weights\n",
    "        else:\n",
    "            return attn_output, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "033e780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, batch_first=False) -> None:\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model,nhead,\n",
    "                                            dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a419510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layer = encoder_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "        for _ in range(self.num_layers):\n",
    "            output = self.layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec5e8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Layer:\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=F.relu,\n",
    "                 layer_norm_eps=1e-5, batch_first=False) -> None:\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model,nhead,\n",
    "                                            dropout=dropout, batch_first=batch_first)\n",
    "        self.multihead_attn = MultiheadAttention(d_model,nhead,dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9146d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layer = decoder_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = tgt\n",
    "        for _ in range(self.num_layers):\n",
    "            output = self.layer(output, memory, tgt_mask=tgt_mask,\n",
    "                                memory_mask=memory_mask,\n",
    "                                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                memory_key_padding_mask=memory_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9156eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625ab39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2f70d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,  d_model: int = 64, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation = F.relu, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False,  reduction=64) -> None:\n",
    "        super(task2Encoder, self).__init__()\n",
    "        self.total_size =8192\n",
    "       \n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first)\n",
    "            encoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "       \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert not (self.total_size % self.d_model), 'd_model needs to be divisible by the size of the entire csi matrix (2048)'\n",
    "        self.feature_shape = (self.total_size//(2*self.d_model), self.d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.fc_encoder = nn.Linear(self.total_size//2,self.total_size//reduction)\n",
    "        #self.fc_decoder = nn.Linear(self.total_size//reduction,self.total_size)\n",
    "        self._reset_parameters()\n",
    "\n",
    "                \n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Optional[Tensor]=None, src_mask: Optional[Tensor] = None,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \n",
    "        memory = self.encoder(src.view(-1, self.feature_shape[0], self.feature_shape[1]), mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        memory_encoder = self.fc_encoder(memory.view(memory.shape[0],-1))\n",
    "        \n",
    "        return memory_encoder\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "   \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf1fbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task2Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,  d_model: int = 64, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation = F.relu, custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False,  reduction=64) -> None:\n",
    "        super(task2Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first)\n",
    "            decoder_norm = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert not (self.total_size % self.d_model), 'd_model needs to be divisible by the size of the entire csi matrix (2048)'\n",
    "        self.feature_shape = (self.total_size//self.d_model, self.d_model)\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        #self.fc_encoder = nn.Linear(self.total_size,self.total_size//reduction)\n",
    "        self.fc_decoder = nn.Linear(self.total_size//reduction,self.total_size)\n",
    "        self._reset_parameters()\n",
    "        \n",
    "        self.decoder_fc2 = nn.Linear(self.total_size, self.total_size//2)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, memory_encoder, tgt: Optional[Tensor]=None, src_mask: Optional[Tensor] = None,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        bs = memory_encoder.size(0)\n",
    "        \n",
    "        # Generate final output\n",
    "        memory_decoder = self.fc_decoder(memory_encoder).view(-1, self.feature_shape[0], self.feature_shape[1])\n",
    "        output = self.decoder(memory_decoder, memory_decoder, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        \n",
    "        output = self.sig2(self.decoder_fc2(output.view(bs, -1)))\n",
    "        \n",
    "        return output.view(bs, -1, 64, 64)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "   \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46df19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 2 model including encoder, decoder and channel\n",
    "class task2model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en=task2Encoder(d_model= 32, num_encoder_layers=2, num_decoder_layers=2, nhead=2, reduction =reduction, dropout= 0.)\n",
    "        \n",
    "        self.de=task2Decoder(d_model=32, num_encoder_layers=2, num_decoder_layers=2, nhead=2, reduction =reduction, dropout= 0.)\n",
    "        \n",
    "    \n",
    "   \n",
    "    def forward(self, Hin, device, is_training): \n",
    "        \n",
    "        #Encoder\n",
    "        Hencoded=self.en(Hin)\n",
    "        \n",
    "        \n",
    "        #Decoder   \n",
    "        Hdecoded=self.de(Hencoded)\n",
    "        \n",
    "\n",
    "        return Hdecoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89ed9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task2weight_path=f'models/TransNettask2/cr{reduction}/gpsFalse_camFalse_radFalse/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7db01c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Encoder, self).__init__()\n",
    "        \n",
    "\n",
    "        #self.en=task2Encoder(reduction)\n",
    "        #reduction value is already considered in the task2weight_path\n",
    "        # loading preloaded values\n",
    "        self.en=torch.load(task2weight_path+\"task2Encoder.pth\")\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the parameters to match the batch size\n",
    "        if self.allow_update:\n",
    "            out = self.en(x)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.en(x)\n",
    "        \n",
    "        encoded_features=out\n",
    "        return encoded_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed0586ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class task3Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=16):\n",
    "        super(task3Decoder, self).__init__()\n",
    "        self.total_size = 8192\n",
    "        w, h =64, 64\n",
    "        self.de = torch.load(task2weight_path+\"task2Decoder.pth\")\n",
    "        \n",
    "        #Layers for auto regression \n",
    "        self.a= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.b= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.c= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.d= nn.Parameter(torch.randn(self.total_size//2))\n",
    "        self.allow_update = False  # Initially, do not allow weight updates\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, Hencoded, input_autoregressive_features):\n",
    "        bs = Hencoded.size(0)\n",
    "        a = self.a.expand(bs, -1)\n",
    "        b = self.b.expand(bs, -1)\n",
    "        c = self.c.expand(bs, -1)\n",
    "        d = self.d.expand(bs, -1)\n",
    "        out_tminus1=input_autoregressive_features[:,0,:].view(bs,-1)\n",
    "        out_tminus2=input_autoregressive_features[:,1,:].view(bs,-1)\n",
    "        if self.allow_update:\n",
    "            out_t = self.de(Hencoded)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out_t = self.de(Hencoded)\n",
    "        #print(out_t.shape)\n",
    "        out = (out_t.view(bs,-1)) * a + out_tminus1 * b + out_tminus2 * c + d\n",
    "        \n",
    "        autoregressive_features = out\n",
    "        \n",
    "        output = out.view(bs,1, 64, 64)\n",
    "        \n",
    "        return output, autoregressive_features\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51999f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete task 3 model including encoder, decoder and channel\n",
    "class task3model(nn.Module):\n",
    "    def __init__(self, reduction=16):\n",
    "        super().__init__()\n",
    "        self.total_size = 8192\n",
    "        self.reduced_size = self.total_size//reduction\n",
    "        self.en = task3Encoder(reduction)\n",
    "        self.de = task3Decoder(reduction)\n",
    "        self.ar = [None] * 5  # List to store the AR variables\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, X, time_index, device, is_training, onoffdict): \n",
    "         \n",
    "        Hin = X\n",
    "        batch_size = Hin.shape[0]\n",
    "        Hencoded = self.en(Hin)\n",
    "        \n",
    "        Hreceived = Hencoded\n",
    "            \n",
    "        # Encoder\n",
    "        if time_index == 0:\n",
    "            iarf = torch.zeros((batch_size, 2, self.total_size//2), dtype=torch.float).to(device)\n",
    "            Hdecoded, self.ar[0] = self.de(Hencoded, iarf)\n",
    "    \n",
    "        elif time_index==1:\n",
    "            iarf=torch.cat([self.ar[0].view(batch_size, 1, self.total_size//2).detach(), torch.zeros((batch_size, 1, self.total_size//2), dtype=torch.float).to(device)], dim=1)\n",
    "            Hdecoded, self.ar[1] = self.de(Hencoded, iarf)\n",
    "            \n",
    "        else:\n",
    "            iarf = torch.cat([self.ar[time_index-1].view(batch_size, 1, self.total_size//2).detach(), self.ar[time_index-2].view(batch_size, 1, self.total_size//2).detach()], dim=1)\n",
    "            Hdecoded, self.ar[time_index] = self.de(Hencoded, iarf)\n",
    "\n",
    "        return Hdecoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38166cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=task3model(reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2ba50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss\n",
    "\n",
    "#criterion=nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion= nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6c938",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7cd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, test_loader, device, criterion):\n",
    "    num_test_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse1 = 0\n",
    "        for b,t_x in enumerate(test_loader):\n",
    "            model.ar = [None] * 5 \n",
    "            for time_index,(X, y) in enumerate(t_x):\n",
    "                y_test_reshaped = CSI_abs_reshape(y.to(device))\n",
    "                Xin = CSI_abs_reshape(X[0].to(device))\n",
    "                # Get the input and output for the given time index\n",
    "                y_pred = model(Xin, time_index, device, is_training=True, onoffdict = onoffdict)\n",
    "                mse0 = criterion(y_pred, y_test_reshaped) \n",
    "                mse1+=mse0  \n",
    "        avg_mse=mse1/(5*num_test_batches)\n",
    "    return avg_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5399da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "585716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DeepVerseChallengeLoaderTaskThree(csv_path = r'./dataset_validation.csv')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70a7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = torch.tensor([])\n",
    "for b,t_x in enumerate(test_loader):\n",
    "        for time_index,(X, y) in enumerate(t_x):\n",
    "            h = CSI_abs_reshape(y)\n",
    "            h_list = torch.cat([h_list,h])\n",
    "target_loss = torch.mean((torch.abs(h_list) - torch.mean(torch.abs(h_list))) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0501436",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c83fa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Improvement Mean Achieved: 31.8025%\n",
      "Percentage Improvement Confidence Interval Achieved: 2.2474%\n",
      "Mean MSE: 0.7698\n",
      "95% Confidence Interval: (0.7444, 0.7951)\n",
      "Margin of Error: 0.0254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_mse_list = []\n",
    "improvement_list = []\n",
    "for _ in range(num_runs):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    model = torch.load(weight_path + \"task3.pth\").to(device)\n",
    "    avg_mse = run_test(model, test_loader, device, criterion)\n",
    "    avg_mse_list.append(avg_mse)\n",
    "    improvement = (target_loss.item() - avg_mse) / target_loss.item() * 100\n",
    "    improvement_list.append(improvement)\n",
    "mean_mse, margin_of_error = calculate_confidence_interval(avg_mse_list)\n",
    "improvement_mean, improvement_margin_of_error = calculate_confidence_interval(improvement_list)\n",
    "print(f'Percentage Improvement Mean Achieved: {improvement_mean:.4f}%')\n",
    "print(f'Percentage Improvement Confidence Interval Achieved: {improvement_margin_of_error:.4f}%')\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"95% Confidence Interval: ({mean_mse - margin_of_error:.4f}, {mean_mse + margin_of_error:.4f})\")\n",
    "print(f\"Margin of Error: {margin_of_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180f32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74073c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
